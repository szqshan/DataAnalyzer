# å®Œæ•´ä¼˜åŒ–ç‰ˆæ•°æ®åº“åˆ†æå™¨ - å®Œæ•´SQLæ‰§è¡Œè®°å½•
# ç‰ˆæœ¬: 2.1.0 - æœ€ç»ˆä¼˜åŒ–ç‰ˆ

from anthropic import Anthropic
import sqlite3
import pandas as pd
import os
from tkinter import filedialog
import tkinter as tk
from datetime import datetime
import json
import re
from typing import Dict, List, Optional, Any

class ConversationMemory:
    """å®Œæ•´è®°å¿†ç®¡ç†ç±» - ä¿å­˜æ‰€æœ‰SQLæ‰§è¡Œè®°å½•å’ŒHTMLå†…å®¹"""
    
    def __init__(self, memory_file="conversation_memory.json"):
        self.conversation_history = []
        self.memory_file = memory_file
        self.load_memory()

    def save_context(self, user_input, ai_response, html_content: str = None, 
                    tool_calls: List = None, analysis_metadata: Dict = None):
        """
        ä¿å­˜å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥ (å­—ç¬¦ä¸²æˆ–å­—å…¸)
            ai_response: AIçš„å®Œæ•´å“åº”å†…å®¹ (å­—ç¬¦ä¸²)
            html_content: ç”Ÿæˆçš„HTMLå†…å®¹
            tool_calls: å®Œæ•´çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
            analysis_metadata: åˆ†æå…ƒæ•°æ®
        """
        timestamp = datetime.now().isoformat()
        
        # è°ƒè¯•è¾“å‡º
        print(f"\nğŸ” ä¿å­˜ä¸Šä¸‹æ–‡ - å·¥å…·è°ƒç”¨åˆ—è¡¨ç±»å‹: {type(tool_calls)}")
        if isinstance(tool_calls, list):
            print(f"ğŸ“Š å·¥å…·è°ƒç”¨æ•°é‡: {len(tool_calls)}")
        
        # å¤„ç†å¯èƒ½æ˜¯å­—å…¸çš„è¾“å…¥
        user_query = ""
        tool_calls_list = []
        if isinstance(user_input, dict):
            user_query = user_input.get("input", "")
            # å¦‚æœtoolså­˜åœ¨äºuser_inputä¸­ï¼Œä½¿ç”¨å®ƒä½œä¸ºå·¥å…·è°ƒç”¨åˆ—è¡¨
            if tool_calls is None and "tools" in user_input:
                tool_calls_list = user_input.get("tools", [])
                print(f"ğŸ“Œ ä»user_inputè·å–å·¥å…·è°ƒç”¨: {len(tool_calls_list)}ä¸ª")
        else:
            user_query = str(user_input)
        
        # å¦‚æœå¤–éƒ¨ä¼ å…¥äº†å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
        if tool_calls is not None:
            tool_calls_list = tool_calls
            print(f"ğŸ“Œ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å·¥å…·è°ƒç”¨: {len(tool_calls_list)}ä¸ª")
            
        # ç¡®ä¿AIå“åº”æ˜¯å­—ç¬¦ä¸²
        ai_response_text = str(ai_response) if ai_response is not None else ""
        
        # ä¿å­˜å®Œæ•´çš„å·¥å…·è°ƒç”¨è®°å½•ï¼ˆåŒ…æ‹¬æ‰€æœ‰SQLæ‰§è¡Œè¯¦æƒ…ï¼‰
        complete_tool_calls = []
        if tool_calls_list:
            for i, tool_call in enumerate(tool_calls_list, 1):
                print(f"  å·¥å…· #{i}: {tool_call.get('tool_name')}")
                complete_tool = {
                    "sequence": i,
                    "tool_name": tool_call.get('tool_name'),
                    "tool_input": tool_call.get('input', {}),
                    "execution_result": tool_call.get('result', {}),
                    "timestamp": datetime.now().isoformat(),
                    "success": 'error' not in tool_call.get('result', {}),
                    "performance": {
                        "execution_time": tool_call.get('result', {}).get('execution_time', 0),
                        "rows_returned": tool_call.get('result', {}).get('row_count', 0)
                    }
                }
                complete_tool_calls.append(complete_tool)
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯è®°å½•
        conversation_entry = {
            "conversation_id": len(self.conversation_history) + 1,
            "timestamp": timestamp,
            "user_interaction": {
                "query": user_query,
                "query_type": self._classify_query(user_query),
                "keywords": self._extract_keywords(user_query)
            },
            "ai_analysis": {
                "full_response": ai_response_text,
                "summary": self._extract_summary(ai_response_text),
                "key_insights": self._extract_insights(ai_response_text),
                "html_output": html_content or "",
                "response_length": len(ai_response_text) if ai_response_text else 0
            },
            "database_operations": {
                "total_tool_calls": len(complete_tool_calls),
                "complete_sql_history": complete_tool_calls,
                "database_info": {
                    "db_path": analysis_metadata.get('database', '') if analysis_metadata else '',
                    "table_name": analysis_metadata.get('table', '') if analysis_metadata else '',
                    "connection_status": "active" if analysis_metadata else "unknown"
                },
                "performance_summary": {
                    "total_execution_time": sum(tc.get('performance', {}).get('execution_time', 0) for tc in complete_tool_calls),
                    "total_rows_processed": sum(tc.get('performance', {}).get('rows_returned', 0) for tc in complete_tool_calls),
                    "successful_queries": sum(1 for tc in complete_tool_calls if tc.get('success', False)),
                    "failed_queries": sum(1 for tc in complete_tool_calls if not tc.get('success', True))
                }
            },
            "output_artifacts": {
                "html_report_file": analysis_metadata.get('html_file', '') if analysis_metadata else '',
                "html_content_size": len(html_content) if html_content else 0,
                "analysis_type": analysis_metadata.get('analysis_type', '') if analysis_metadata else 'unknown',
                "report_generated": bool(html_content)
            },
            "session_metadata": {
                "iterations_count": analysis_metadata.get('iterations', 0) if analysis_metadata else 0,
                "analysis_duration": self._calculate_duration(),
                "completion_status": "completed" if not analysis_metadata or analysis_metadata.get('iterations', 0) > 0 else "incomplete"
            }
        }
        
        self.conversation_history.append(conversation_entry)
        self.save_memory()
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿å­˜çŠ¶æ€
        print(f"\nğŸ’¾ å®Œæ•´è®°å¿†å·²ä¿å­˜:")
        print(f"  ğŸ†” å¯¹è¯ID: {conversation_entry['conversation_id']}")
        print(f"  ğŸ”§ å·¥å…·è°ƒç”¨: {len(complete_tool_calls)}æ¬¡")
        print(f"  ğŸ—ƒï¸  SQLæŸ¥è¯¢: {conversation_entry['database_operations']['performance_summary']['successful_queries']}æ¬¡æˆåŠŸ")
        if html_content:
            print(f"  ğŸ“„ HTMLå†…å®¹: {len(html_content):,}å­—ç¬¦")
        print(f"  â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {conversation_entry['database_operations']['performance_summary']['total_execution_time']:.3f}ç§’")
        print(f"  ğŸ“Š å¤„ç†æ•°æ®: {conversation_entry['database_operations']['performance_summary']['total_rows_processed']:,}è¡Œ")
    
    def _classify_query(self, query: str) -> str:
        """åˆ†ç±»ç”¨æˆ·æŸ¥è¯¢ç±»å‹"""
        if not query:
            return "general"
            
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in ['html', 'æŠ¥å‘Š', 'å¯è§†åŒ–', 'å›¾è¡¨']):
            return "report_generation"
        elif any(keyword in query_lower for keyword in ['åˆ†æ', 'ç»Ÿè®¡', 'æ´å¯Ÿ']):
            return "data_analysis"
        elif any(keyword in query_lower for keyword in ['ä¼˜åŒ–', 'æ”¹è¿›', 'ä¿®æ”¹']):
            return "optimization"
        elif any(keyword in query_lower for keyword in ['åŸºç¡€', 'æ¦‚è§ˆ', 'æ€»ç»“']):
            return "overview"
        else:
            return "general"
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        if not text:
            return []
            
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [word for word in words if len(word) > 2 and word not in ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'does', 'let', 'put', 'say', 'she', 'too', 'use']]
        return list(set(keywords))[:10]  # è¿”å›å‰10ä¸ªå”¯ä¸€å…³é”®è¯
    
    def _extract_summary(self, ai_response: str) -> str:
        """ä»AIå“åº”ä¸­æå–æ‘˜è¦"""
        if not ai_response:
            return "æ•°æ®åˆ†ææŠ¥å‘Š"
            
        lines = ai_response.split('\n')
        summary_parts = []
        
        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in ['## ', '### ', '#### ', 'æ€»ç»“', 'æ‘˜è¦', 'å…³é”®å‘ç°', 'æ ¸å¿ƒæ´å¯Ÿ', 'åˆ†ææŠ¥å‘Š']):
                clean_line = re.sub(r'^#+\s*', '', line)
                if len(clean_line) > 5:
                    summary_parts.append(clean_line)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨å‰å‡ å¥è¯
        if not summary_parts:
            sentences = [s.strip() for s in ai_response.split('.') if len(s.strip()) > 20]
            summary_parts = sentences[:2]
                
        return ' | '.join(summary_parts[:3]) if summary_parts else "æ•°æ®åˆ†ææŠ¥å‘Š"
    
    def _extract_insights(self, ai_response: str) -> List[str]:
        """æå–å…³é”®æ´å¯Ÿ"""
        if not ai_response:
            return []
            
        insights = []
        lines = ai_response.split('\n')
        
        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in ['å»ºè®®', 'ä¼˜åŒ–', 'æ”¹è¿›', 'æ³¨æ„', 'å…³é”®', 'å‘ç°', 'æ´å¯Ÿ', 'é‡è¦']):
                # æ¸…ç†æ ¼å¼æ ‡è®°
                clean_line = re.sub(r'^[-*â€¢]\s*', '', line)
                clean_line = re.sub(r'^\d+\.\s*', '', clean_line)
                if len(clean_line) > 15:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    insights.append(clean_line)
        
        return insights[:8]  # æœ€å¤šä¿å­˜8æ¡æ´å¯Ÿ
    
    def _calculate_duration(self) -> str:
        """è®¡ç®—åˆ†ææŒç»­æ—¶é—´"""
        return f"{datetime.now().strftime('%H:%M:%S')}"
    
    def get_relevant_context(self, current_query: str, max_results: int = 2) -> str:
        """è·å–ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡"""
        if not self.conversation_history:
            return ""
        
        current_keywords = set(self._extract_keywords(current_query))
        relevant_conversations = []
        
        for entry in self.conversation_history:
            # è®¡ç®—å…³é”®è¯é‡å åº¦
            entry_keywords = set(entry['user_interaction']['keywords'])
            keyword_overlap = len(current_keywords & entry_keywords)
            
            # è®¡ç®—æŸ¥è¯¢ç±»å‹åŒ¹é…åº¦
            query_type_match = 1 if entry['user_interaction']['query_type'] == self._classify_query(current_query) else 0
            
            # ç»¼åˆç›¸å…³åº¦è¯„åˆ†
            relevance_score = keyword_overlap + (query_type_match * 2)
            
            if relevance_score > 0:
                relevant_conversations.append((entry, relevance_score))
        
        if not relevant_conversations:
            return ""
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        relevant_conversations.sort(key=lambda x: x[1], reverse=True)
        
        # æ„å»ºè¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = ["=== ğŸ” ç›¸å…³å†å²åˆ†æ ==="]
        for entry, score in relevant_conversations[:max_results]:
            context_parts.append(f"ğŸ“… å¯¹è¯ID {entry['conversation_id']} | {entry['timestamp'][:16]}")
            context_parts.append(f"â“ é—®é¢˜: {entry['user_interaction']['query']}")
            context_parts.append(f"ğŸ“Š æ‘˜è¦: {entry['ai_analysis']['summary']}")
            context_parts.append(f"ğŸ”§ å·¥å…·è°ƒç”¨: {entry['database_operations']['total_tool_calls']}æ¬¡")
            
            # æ˜¾ç¤ºå…³é”®SQLæ“ä½œ
            if entry['database_operations']['complete_sql_history']:
                context_parts.append(f"ğŸ—ƒï¸  ä¸»è¦SQLæ“ä½œ:")
                for i, sql_op in enumerate(entry['database_operations']['complete_sql_history'][:3], 1):
                    sql_text = sql_op['tool_input'].get('sql', sql_op['tool_name'])
                    if sql_text and sql_text != sql_op['tool_name']:
                        context_parts.append(f"   {i}. {sql_text[:80]}...")
                    else:
                        context_parts.append(f"   {i}. {sql_op['tool_name']}")
            
            if entry['ai_analysis']['html_output']:
                context_parts.append(f"ğŸ“„ HTMLæŠ¥å‘Š: {entry['output_artifacts']['html_content_size']:,}å­—ç¬¦")
            
            context_parts.append("")
        
        return "\n".join(context_parts)
    
    def get_html_content(self, conversation_id: int = None) -> str:
        """è·å–æŒ‡å®šå¯¹è¯çš„HTMLå†…å®¹"""
        if conversation_id is None and self.conversation_history:
            # è·å–æœ€æ–°çš„HTMLå†…å®¹
            latest = self.conversation_history[-1]
            return latest.get('ai_analysis', {}).get('html_output', '')
        
        for entry in self.conversation_history:
            if entry['conversation_id'] == conversation_id:
                return entry.get('ai_analysis', {}).get('html_output', '')
        
        return ""
    
    def get_sql_history(self, conversation_id: int = None) -> List[Dict]:
        """è·å–SQLæ‰§è¡Œå†å²"""
        if conversation_id is None:
            # è¿”å›æ‰€æœ‰SQLå†å²
            all_sql = []
            for entry in self.conversation_history:
                for sql_op in entry['database_operations']['complete_sql_history']:
                    sql_op['conversation_id'] = entry['conversation_id']
                    sql_op['conversation_timestamp'] = entry['timestamp']
                    all_sql.append(sql_op)
            return all_sql
        else:
            # è¿”å›æŒ‡å®šå¯¹è¯çš„SQLå†å²
            for entry in self.conversation_history:
                if entry['conversation_id'] == conversation_id:
                    return entry['database_operations']['complete_sql_history']
            return []
    
    def save_memory(self):
        """ä¿å­˜è®°å¿†åˆ°æ–‡ä»¶ - å®Œæ•´æ ¼å¼"""
        try:
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_sql_calls = sum(entry['database_operations']['total_tool_calls'] for entry in self.conversation_history)
            total_html_reports = sum(1 for entry in self.conversation_history if entry['ai_analysis']['html_output'])
            total_successful_queries = sum(entry['database_operations']['performance_summary']['successful_queries'] for entry in self.conversation_history)
            total_failed_queries = sum(entry['database_operations']['performance_summary']['failed_queries'] for entry in self.conversation_history)
            
            memory_data = {
                "metadata": {
                    "version": "2.1.0",
                    "description": "å®Œæ•´SQLæ‰§è¡Œè®°å½•å’ŒHTMLå†…å®¹ä¿å­˜",
                    "created": datetime.now().isoformat(),
                    "total_conversations": len(self.conversation_history),
                    "last_updated": datetime.now().isoformat(),
                    "file_format": "complete_memory_with_full_sql_history"
                },
                "conversations": self.conversation_history,
                "global_statistics": {
                    "total_sql_operations": total_sql_calls,
                    "successful_sql_queries": total_successful_queries,
                    "failed_sql_queries": total_failed_queries,
                    "html_reports_generated": total_html_reports,
                    "avg_iterations_per_conversation": sum(entry['session_metadata']['iterations_count'] for entry in self.conversation_history) / len(self.conversation_history) if self.conversation_history else 0,
                    "total_data_rows_processed": sum(entry['database_operations']['performance_summary']['total_rows_processed'] for entry in self.conversation_history),
                    "total_execution_time": sum(entry['database_operations']['performance_summary']['total_execution_time'] for entry in self.conversation_history)
                },
                "schema_info": {
                    "conversation_structure": {
                        "conversation_id": "Unique identifier for each conversation",
                        "user_interaction": "Complete user query information",
                        "ai_analysis": "Full AI response and HTML content",
                        "database_operations": "Complete SQL execution history with results",
                        "output_artifacts": "Generated files and reports",
                        "session_metadata": "Analysis session information"
                    }
                }
            }
            
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(memory_data, f, ensure_ascii=False, indent=2)
                
            print(f"ğŸ’¾ è®°å¿†æ–‡ä»¶å·²ä¿å­˜: {self.memory_file}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(self.memory_file):,} å­—èŠ‚")
                
        except Exception as e:
            print(f"ğŸ’¥ è®°å¿†ä¿å­˜å¤±è´¥: {e}")
    
    def load_memory(self):
        """ä»æ–‡ä»¶åŠ è½½è®°å¿†"""
        try:
            if os.path.exists(self.memory_file):
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    memory_data = json.load(f)
                
                # å…¼å®¹æ–°æ—§æ ¼å¼
                if "conversations" in memory_data:
                    self.conversation_history = memory_data["conversations"]
                    stats = memory_data.get("global_statistics", {})
                    print(f"âœ… å®Œæ•´è®°å¿†åŠ è½½æˆåŠŸ:")
                    print(f"   ğŸ“Š å¯¹è¯æ•°: {len(self.conversation_history)}")
                    print(f"   ğŸ—ƒï¸  SQLæ“ä½œ: {stats.get('total_sql_operations', 0)}æ¬¡")
                    print(f"   ğŸ“„ HTMLæŠ¥å‘Š: {stats.get('html_reports_generated', 0)}ä¸ª")
                    print(f"   ğŸ“ˆ æ•°æ®è¡Œæ•°: {stats.get('total_data_rows_processed', 0):,}è¡Œ")
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    old_history = memory_data.get("conversation_history", [])
                    print(f"âš ï¸  æ£€æµ‹åˆ°æ—§æ ¼å¼ï¼Œå·²å…¼å®¹åŠ è½½: {len(old_history)}æ¡è®°å½•")
                    self.conversation_history = []
                    
        except Exception as e:
            print(f"âš ï¸  è®°å¿†åŠ è½½å¤±è´¥: {e}")
            self.conversation_history = []
    
    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        backup_name = f"{self.memory_file}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        if os.path.exists(self.memory_file):
            # åˆ›å»ºå¤‡ä»½
            import shutil
            shutil.copy2(self.memory_file, backup_name)
            os.remove(self.memory_file)
        
        self.conversation_history = []
        print("ğŸ—‘ï¸  è®°å¿†å·²æ¸…ç©º")
        print(f"ğŸ’¾ å¤‡ä»½æ–‡ä»¶: {backup_name}")
    
    def show_conversation_list(self):
        """æ˜¾ç¤ºè¯¦ç»†å¯¹è¯åˆ—è¡¨"""
        if not self.conversation_history:
            print("ğŸ“ æš‚æ— å¯¹è¯è®°å½•")
            return
            
        print("\nğŸ“š å¯¹è¯å†å²è¯¦ç»†åˆ—è¡¨:")
        print("=" * 100)
        
        for entry in self.conversation_history:
            print(f"ğŸ†” ID: {entry['conversation_id']} | â° {entry['timestamp'][:19]} | ğŸ¯ {entry['user_interaction']['query_type']}")
            print(f"â“ é—®é¢˜: {entry['user_interaction']['query'][:70]}...")
            print(f"ğŸ“Š æ‘˜è¦: {entry['ai_analysis']['summary'][:80]}...")
            print(f"ğŸ”§ æ“ä½œ: {entry['database_operations']['total_tool_calls']}æ¬¡å·¥å…·è°ƒç”¨ | " +
                  f"âœ… {entry['database_operations']['performance_summary']['successful_queries']}æ¬¡æˆåŠŸ | " +
                  f"âŒ {entry['database_operations']['performance_summary']['failed_queries']}æ¬¡å¤±è´¥")
            print(f"ğŸ“„ HTML: {'âœ… ' + str(entry['output_artifacts']['html_content_size']) + 'å­—ç¬¦' if entry['ai_analysis']['html_output'] else 'âŒ'}")
            print(f"â±ï¸  æ‰§è¡Œ: {entry['database_operations']['performance_summary']['total_execution_time']:.3f}ç§’ | " +
                  f"ğŸ“Š æ•°æ®: {entry['database_operations']['performance_summary']['total_rows_processed']:,}è¡Œ")
            print("-" * 100)
    
    def show_sql_details(self, conversation_id: int = None):
        """æ˜¾ç¤ºè¯¦ç»†SQLæ‰§è¡Œè®°å½•"""
        sql_history = self.get_sql_history(conversation_id)
        
        if not sql_history:
            print("ğŸ—ƒï¸  æ— SQLæ‰§è¡Œè®°å½•")
            return
        
        print(f"\nğŸ—ƒï¸  SQLæ‰§è¡Œè¯¦ç»†è®°å½• {'(å¯¹è¯ID: ' + str(conversation_id) + ')' if conversation_id else '(å…¨éƒ¨)'}")
        print("=" * 120)
        
        for sql_op in sql_history:
            print(f"ğŸ”„ åºå·: {sql_op.get('sequence', 'N/A')} | â° {sql_op.get('timestamp', '')[:19]}")
            print(f"ğŸ”§ å·¥å…·: {sql_op.get('tool_name', 'unknown')}")
            
            if sql_op.get('tool_input', {}).get('sql'):
                print(f"ğŸ“ SQL: {sql_op['tool_input']['sql']}")
            
            print(f"ğŸ“Š ç»“æœ: {'âœ… æˆåŠŸ' if sql_op.get('success') else 'âŒ å¤±è´¥'} | " +
                  f"â±ï¸ {sql_op.get('performance', {}).get('execution_time', 0):.3f}ç§’ | " +
                  f"ğŸ“ˆ {sql_op.get('performance', {}).get('rows_returned', 0)}è¡Œ")
            
            # æ˜¾ç¤ºç»“æœè¯¦æƒ…
            if sql_op.get('execution_result'):
                result = sql_op['execution_result']
                if 'columns' in result:
                    print(f"ğŸ“‹ åˆ—å: {', '.join(result['columns'][:5])}{'...' if len(result['columns']) > 5 else ''}")
                if 'error' in result:
                    print(f"âŒ é”™è¯¯: {result['error']}")
            
            print("-" * 120)
    
    def export_html_report(self, conversation_id: int, output_path: str = None):
        """å¯¼å‡ºæŒ‡å®šå¯¹è¯çš„HTMLæŠ¥å‘Š"""
        html_content = self.get_html_content(conversation_id)
        if not html_content:
            print(f"âŒ å¯¹è¯ {conversation_id} æ²¡æœ‰HTMLå†…å®¹")
            return False
            
        if output_path is None:
            output_path = f"exported_report_{conversation_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"ğŸ“„ HTMLæŠ¥å‘Šå·²å¯¼å‡º: {output_path}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_path):,} å­—èŠ‚")
            return True
        except Exception as e:
            print(f"ğŸ’¥ å¯¼å‡ºå¤±è´¥: {e}")
            return False

    def get_memory_summary(self):
        """è·å–è®°å¿†æ‘˜è¦"""
        if not self.conversation_history:
            return {"conversation_count": 0}
            
        latest = self.conversation_history[-1]
        total_sql = sum(entry['database_operations']['total_tool_calls'] for entry in self.conversation_history)
        
        return {
            "conversation_count": len(self.conversation_history),
            "memory_file": self.memory_file,
            "latest_timestamp": latest["timestamp"],
            "latest_query": latest["user_interaction"]["query"],
            "has_html": bool(latest["ai_analysis"]["html_output"]),
            "total_sql_calls": total_sql,
            "latest_html_size": latest["output_artifacts"]["html_content_size"]
        }


class DatabaseAnalyzer:
    """æ•°æ®åº“åˆ†æå™¨ç±»"""
    
    def __init__(self, api_key, model_name="claude-sonnet-4-20250514", base_url=None):
        """
        åˆå§‹åŒ–æ•°æ®åº“åˆ†æå™¨
        
        Args:
            api_key: APIå¯†é’¥
            model_name: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
        """
        client_params = {"api_key": api_key}
        if base_url:
            client_params["base_url"] = base_url
            print(f"ğŸ”§ DatabaseAnalyzeråˆå§‹åŒ–: ä½¿ç”¨è‡ªå®šä¹‰base_url={base_url}")
        else:
            print(f"ğŸ”§ DatabaseAnalyzeråˆå§‹åŒ–: æœªæä¾›base_urlï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
            
        self.client = Anthropic(**client_params)
        print(f"ğŸ”§ Anthropicå®¢æˆ·ç«¯åˆ›å»ºå®Œæˆï¼Œå®é™…ä½¿ç”¨çš„base_url: {self.client.base_url}")
        
        self.model_name = model_name
        self.debug_mode = False
        self.current_db_path = None
        self.current_table_name = None
        self.memory = ConversationMemory()
        
        # å®šä¹‰å·¥å…· - ä½¿ç”¨customç±»å‹
        self.tools = [
            {
                "type": "custom",
                "name": "query_database",
                "description": "æ‰§è¡ŒSQLæŸ¥è¯¢è·å–æ•°æ®åº“ä¿¡æ¯",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "sql": {
                            "type": "string",
                            "description": "è¦æ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥"
                        }
                    },
                    "required": ["sql"]
                }
            },
            {
                "type": "custom",
                "name": "get_table_info",
                "description": "è·å–è¡¨çš„ç»“æ„ä¿¡æ¯å’Œæ ·æœ¬æ•°æ®",
                "input_schema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }
        ]
        
    def set_debug_mode(self, debug=True):
        """è®¾ç½®è°ƒè¯•æ¨¡å¼"""
        self.debug_mode = debug
        
    def debug_print(self, message, level="INFO"):
        """è°ƒè¯•ä¿¡æ¯è¾“å‡º"""
        if self.debug_mode:
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] [{level}] {message}")
            
    def debug_print_chunk(self, chunk):
        """è¯¦ç»†æ‰“å°æ¨¡å‹å“åº”å—çš„å†…å®¹"""
        if not self.debug_mode:
            return
            
        try:
            print("\n=== æ¨¡å‹å“åº”å—è¯¦æƒ… ===")
            print(f"ç±»å‹: {chunk.type}")
            
            if hasattr(chunk, 'content_block') and chunk.type == "content_block_start":
                print(f"å†…å®¹å—ç±»å‹: {chunk.content_block.type}")
                if chunk.content_block.type == "tool_use":
                    print(f"å·¥å…·åç§°: {chunk.content_block.name}")
                    print(f"å·¥å…·ID: {chunk.content_block.id}")
            
            if hasattr(chunk, 'delta'):
                if hasattr(chunk.delta, 'type'):
                    print(f"Deltaç±»å‹: {chunk.delta.type}")
                    
                    if chunk.delta.type == "tool_use" and hasattr(chunk.delta, 'tool_use'):
                        if hasattr(chunk.delta.tool_use, 'name'):
                            print(f"å·¥å…·åç§°: {chunk.delta.tool_use.name}")
                        if hasattr(chunk.delta.tool_use, 'input'):
                            print(f"å·¥å…·è¾“å…¥: {chunk.delta.tool_use.input}")
                    elif chunk.delta.type == "text_delta":
                        print(f"æ–‡æœ¬å†…å®¹: {chunk.delta.text[:50]}...")
                    elif chunk.delta.type == "input_json_delta":
                        print(f"JSONè¾“å…¥: {chunk.delta.partial_json[:50]}...")
                else:
                    print(f"Deltaæ²¡æœ‰typeå±æ€§: {vars(chunk.delta) if hasattr(chunk.delta, '__dict__') else 'No __dict__'}")
            
            print("======================")
        except Exception as e:
            print(f"è°ƒè¯•æ‰“å°é”™è¯¯: {str(e)}")
            if self.debug_mode:
                import traceback
                traceback.print_exc()
    
    def import_csv_to_sqlite(self, csv_file_path, table_name, db_path="analysis_db.db"):
        """ä»CSVæ–‡ä»¶åˆ›å»ºSQLiteè¡¨å¹¶å¯¼å…¥æ•°æ®"""
        try:
            self.debug_print(f"å¼€å§‹å¯¼å…¥CSVæ–‡ä»¶: {csv_file_path}")
            
            if not os.path.exists(csv_file_path):
                return {"success": False, "message": f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}"}
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file_path, encoding='utf-8')
            self.debug_print(f"CSVæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")
            
            # æ¸…ç†åˆ—å
            df.columns = [self._clean_column_name(col) for col in df.columns]
            
            # è¿æ¥åˆ°SQLiteæ•°æ®åº“
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # åˆ é™¤å·²å­˜åœ¨çš„è¡¨å¹¶åˆ›å»ºæ–°è¡¨
            cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
            df.to_sql(table_name, conn, if_exists='replace', index=False)
            
            # è·å–å¯¼å…¥çš„è¡Œæ•°
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            rows_count = cursor.fetchone()[0]
            
            conn.commit()
            conn.close()
            
            # ä¿å­˜å½“å‰æ•°æ®åº“ä¿¡æ¯
            self.current_db_path = db_path
            self.current_table_name = table_name
            
            print(f"ğŸ—‘ï¸  æ¸…ç©ºä¹‹å‰çš„è®°å¿†ä»¥å¼€å§‹æ–°çš„åˆ†æä¼šè¯")
            self.memory.clear_memory()
            
            return {
                "success": True,
                "message": f"æˆåŠŸå¯¼å…¥ {rows_count} è¡Œæ•°æ®åˆ°è¡¨ '{table_name}'",
                "rows_imported": rows_count,
                "columns": list(df.columns)
            }
            
        except Exception as e:
            self.debug_print(f"å¯¼å…¥å¤±è´¥: {str(e)}", "ERROR")
            return {"success": False, "message": f"å¯¼å…¥å¤±è´¥: {str(e)}"}
    
    def _clean_column_name(self, col_name):
        """æ¸…ç†åˆ—å"""
        cleaned = str(col_name).strip()
        cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '_', cleaned)
        cleaned = re.sub(r'_+', '_', cleaned)
        cleaned = cleaned.strip('_')
        return cleaned or 'unnamed_column'
    
    def get_table_schema(self):
        """è·å–æ•°æ®åº“è¡¨çš„ç»“æ„ä¿¡æ¯"""
        if not self.current_db_path or not self.current_table_name:
            return "æœªè¿æ¥åˆ°æ•°æ®åº“"
        
        try:
            conn = sqlite3.connect(self.current_db_path)
            cursor = conn.cursor()
            
            # è·å–è¡¨ç»“æ„
            schema_info = cursor.execute(f"PRAGMA table_info({self.current_table_name})").fetchall()
            
            # è·å–æ ·æœ¬æ•°æ®
            sample_data = cursor.execute(f"SELECT * FROM {self.current_table_name} LIMIT 3").fetchall()
            column_names = [description[0] for description in cursor.description]
            
            conn.close()
            
            # æ„å»ºç»“æ„æè¿°
            schema = {
                "table_name": self.current_table_name,
                "columns": [{"name": col[1], "type": col[2]} for col in schema_info],
                "sample_data": [dict(zip(column_names, row)) for row in sample_data]
            }
            
            return schema
            
        except Exception as e:
            return f"è·å–è¡¨ç»“æ„å¤±è´¥: {str(e)}"
    
    def query_database(self, sql):
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        if not self.current_db_path:
            return {"error": "æœªè¿æ¥åˆ°æ•°æ®åº“"}
        
        # è¾“å‡ºæ­£åœ¨æ‰§è¡Œçš„SQL
        print(f"\nğŸ—ƒï¸  æ­£åœ¨æ‰§è¡ŒSQLæŸ¥è¯¢:")
        print(f"   {sql}")
        
        try:
            conn = sqlite3.connect(self.current_db_path)
            cursor = conn.cursor()
            
            start_time = datetime.now()
            cursor.execute(sql)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            if sql.strip().upper().startswith('SELECT'):
                results = cursor.fetchall()
                columns = [description[0] for description in cursor.description]
                
                result_data = {
                    "columns": columns,
                    "data": results,
                    "row_count": len(results),
                    "sql_executed": sql,
                    "execution_time": execution_time,
                    "query_type": "SELECT",
                    "data_preview": results[:5] if results else []
                }
                
                print(f"âœ… æŸ¥è¯¢æˆåŠŸ: è¿”å› {len(results)} è¡Œ x {len(columns)} åˆ— (è€—æ—¶: {execution_time:.3f}s)")
                if len(results) > 0:
                    print(f"ğŸ“‹ åˆ—å: {', '.join(columns[:5])}{'...' if len(columns) > 5 else ''}")
                
                conn.close()
                return result_data
            else:
                conn.commit()
                conn.close()
                print(f"âœ… éæŸ¥è¯¢è¯­å¥æ‰§è¡ŒæˆåŠŸ (è€—æ—¶: {execution_time:.3f}s)")
                return {
                    "message": "æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ", 
                    "sql_executed": sql, 
                    "execution_time": execution_time,
                    "query_type": "NON_SELECT"
                }
                
        except Exception as e:
            error_msg = f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                "error": error_msg, 
                "sql_attempted": sql,
                "execution_time": 0,
                "query_type": "ERROR"
            }
    
    def execute_tool(self, tool_name, tool_input):
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        self.debug_print(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_input}")
        
        try:
            if tool_name == "query_database":
                sql = tool_input.get("sql", "")
                if not sql:
                    print("âš ï¸  è­¦å‘Š: SQLå‚æ•°ä¸ºç©º")
                    return {"error": "SQLå‚æ•°ä¸ºç©º", "query_type": "ERROR"}
                return self.query_database(sql)
            
            elif tool_name == "get_table_info":
                print(f"ğŸ” è·å–è¡¨ç»“æ„ä¿¡æ¯: {self.current_table_name}")
                result = self.get_table_schema()
                print(f"âœ… è¡¨ç»“æ„è·å–å®Œæˆ")
                return {
                    "table_info": result,
                    "query_type": "TABLE_INFO",
                    "execution_time": 0.001
                }
            
            else:
                print(f"âŒ æœªçŸ¥å·¥å…·: {tool_name}")
                return {"error": f"æœªçŸ¥å·¥å…·: {tool_name}", "query_type": "ERROR"}
        except Exception as e:
            error_msg = f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return {
                "error": error_msg,
                "query_type": "ERROR",
                "execution_time": 0
            }
    
    def analyze_with_llm(self, user_query):
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ•°æ®åˆ†æ - å®Œæ•´SQLè®°å½•ç‰ˆæœ¬"""
        if not self.current_db_path or not self.current_table_name:
            return "è¯·å…ˆå¯¼å…¥æ•°æ®"
        
        self.debug_print(f"å¼€å§‹æ™ºèƒ½åˆ†æ: {user_query}")
        
        # åˆ›å»ºHTMLè¾“å‡ºæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        html_filename = f"analysis_report_{timestamp}.html"
        
        # è·å–ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡
        conversation_context = self.memory.get_relevant_context(user_query)
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šæ•°æ®åˆ†æå¸ˆï¼Œä¸“é—¨ç”Ÿæˆä¸“å®¶çº§ç¾å·¥è´¨é‡çš„HTMLæ•°æ®åˆ†ææŠ¥å‘Šã€‚

        {conversation_context}

        æ•°æ®åº“ä¿¡æ¯ï¼š
        - è·¯å¾„: {self.current_db_path}
        - è¡¨å: {self.current_table_name}

        ğŸ”§ å¯ç”¨å·¥å…·ï¼š
        1. query_database: æ‰§è¡ŒSQLæŸ¥è¯¢
        2. get_table_info: è·å–è¡¨ç»“æ„

        ğŸ“‹ åˆ†ææµç¨‹ï¼š
        **é˜¶æ®µ1ï¼šæ•°æ®æ¢ç´¢** (2-3æ¬¡æŸ¥è¯¢)
        - è·å–æ€»è®°å½•æ•°å’Œè¡¨ç»“æ„
        - æ£€æŸ¥æ ¸å¿ƒå­—æ®µå®Œæ•´æ€§

        **é˜¶æ®µ2ï¼šç»´åº¦åˆ†æ** (3-5æ¬¡æŸ¥è¯¢)  
        - ä¸»è¦åˆ†ç±»å­—æ®µåˆ†å¸ƒç»Ÿè®¡
        - æ•°å€¼å­—æ®µåŸºç¡€ç»Ÿè®¡
        - äº¤å‰éªŒè¯æ•°æ®ä¸€è‡´æ€§

        **é˜¶æ®µ3ï¼šæ·±åº¦æ´å¯Ÿ** (2-3æ¬¡æŸ¥è¯¢)
        - å¼‚å¸¸å€¼æ£€æµ‹
        - å¤šç»´åº¦å…³è”åˆ†æ

        ğŸ“Š HTMLè¾“å‡ºè¦æ±‚ï¼š
        **å¿…é¡»å®Œæ•´HTMLæ–‡æ¡£** (<!DOCTYPE html>å¼€å¤´åˆ°</html>ç»“å°¾)
        - Chart.js 3.9.1 äº¤äº’å›¾è¡¨
        - ä¸“å®¶çº§CSSè®¾è®¡
        - å“åº”å¼è®¾è®¡
        - åŠ¨æ€æ•ˆæœå’Œäº¤äº’
        - è¦ç¡®ä¿æ‰€æœ‰chartèƒ½å¤Ÿæ­£å¸¸æ˜¾ç¤º

        è¯·åŸºäºæ•°æ®åˆ†æç”Ÿæˆå®Œæ•´çš„ä¸“å®¶çº§HTMLæ•°æ®åˆ†ææŠ¥å‘Šã€‚"""

        # ç”¨äºæ”¶é›†å®Œæ•´çš„AIå“åº”
        full_ai_response = []
        html_content = ""
        
        # åˆ›å»ºHTMLæ–‡ä»¶
        with open(html_filename, 'w', encoding='utf-8') as f:
            f.write("")
        
        def write_to_html(content):
            """å†™å…¥HTMLæ–‡ä»¶å¹¶æ”¶é›†å†…å®¹"""
            nonlocal html_content
            html_content += content
            with open(html_filename, 'a', encoding='utf-8') as f:
                f.write(content)
                f.flush()
        
        print(f"ğŸ“„ æ­£åœ¨ç”ŸæˆHTMLæŠ¥å‘Š: {html_filename}")
        
        # åˆå§‹æ¶ˆæ¯
        messages = [
            {"role": "user", "content": f"{system_prompt}\n\nç”¨æˆ·éœ€æ±‚: {user_query}"}
        ]
        
        # æ ¹æ®ç”¨æˆ·æŸ¥è¯¢åˆ¤æ–­åˆ†ææ·±åº¦
        query_lower = user_query.lower()
        if any(keyword in query_lower for keyword in ['ç®€å•', 'åŸºç¡€', 'æ¦‚è§ˆ', 'å¿«é€Ÿ']):
            max_iterations = 15
            analysis_type = "ç®€å•åˆ†æ"
        elif any(keyword in query_lower for keyword in ['æ·±åº¦', 'è¯¦ç»†', 'å…¨é¢', 'å®Œæ•´']):
            max_iterations = 30
            analysis_type = "æ·±åº¦åˆ†æ"
        else:
            max_iterations = 50
            analysis_type = "æ ‡å‡†åˆ†æ"
        
        print(f"ğŸ¯ åˆ†æç±»å‹: {analysis_type} (æœ€å¤§{max_iterations}è½®)")
        
        iteration = 0
        tool_calls_made = []
        
        while iteration < max_iterations:
            iteration += 1
            self.debug_print(f"ç¬¬{iteration}è½®å¯¹è¯")
            
            try:
                print(f"\nğŸ”„ ç¬¬{iteration}è½®åˆ†æä¸­...", end="", flush=True)
                
                # è°ƒç”¨Claude API (æµå¼è¾“å‡º)
                response = self.client.messages.create(
                    model=self.model_name,
                    max_tokens=40000,
                    messages=messages,
                    tools=self.tools,
                    stream=True
                )
                
                # æ­£ç¡®å¤„ç†æµå¼å“åº”
                assistant_response = {"role": "assistant", "content": []}
                current_tool_inputs = {}  # ç”¨äºç´¯ç§¯å·¥å…·å‚æ•°
                current_text_response = ""  # æ”¶é›†æ–‡æœ¬å“åº”
                
                print(f" ğŸ“ æ­£åœ¨å†™å…¥HTML...")
                
                for chunk in response:
                    # ä½¿ç”¨è°ƒè¯•å‡½æ•°æ‰“å°è¯¦ç»†ä¿¡æ¯
                    self.debug_print_chunk(chunk)
                    
                    if chunk.type == "message_start":
                        continue
                    elif chunk.type == "content_block_start":
                        if chunk.content_block.type == "text":
                            assistant_response["content"].append({"type": "text", "text": ""})
                        elif chunk.content_block.type == "tool_use":
                            print(f"\nğŸ”§ å·¥å…·è°ƒç”¨å¼€å§‹: {chunk.content_block.name}")
                            tool_block = {
                                "type": "tool_use",
                                "id": chunk.content_block.id,
                                "name": chunk.content_block.name,
                                "input": {}
                            }
                            assistant_response["content"].append(tool_block)
                            current_tool_inputs[chunk.content_block.id] = ""
                    elif chunk.type == "content_block_delta":
                        try:
                            if chunk.delta.type == "text_delta":
                                text_content = chunk.delta.text
                                assistant_response["content"][-1]["text"] += text_content
                                current_text_response += text_content
                                write_to_html(text_content)
                                print(".", end="", flush=True)
                            elif chunk.delta.type == "input_json_delta":
                                print(f"\nğŸ“„ JSONè¾“å…¥å¢é‡: {chunk.delta.partial_json[:30]}...")
                                if assistant_response["content"] and assistant_response["content"][-1].get("type") == "tool_use":
                                    tool_id = assistant_response["content"][-1]["id"]
                                    if tool_id in current_tool_inputs:
                                        current_tool_inputs[tool_id] += chunk.delta.partial_json
                        except Exception as e:
                            print(f"\nâš ï¸ å¤„ç†å“åº”å—é”™è¯¯: {str(e)}")
                            import traceback
                            traceback.print_exc()
                    elif chunk.type == "content_block_stop":
                        if assistant_response["content"] and assistant_response["content"][-1].get("type") == "tool_use":
                            tool_id = assistant_response["content"][-1]["id"]
                            if tool_id in current_tool_inputs:
                                try:
                                    json_str = current_tool_inputs[tool_id]
                                    # å¤„ç†å¯èƒ½çš„ä¸å®Œæ•´JSON
                                    if json_str.strip():
                                        if not (json_str.strip().startswith('{') or json_str.strip().startswith('[')):
                                            json_str = '{' + json_str + '}'
                                        complete_input = json.loads(json_str)
                                        assistant_response["content"][-1]["input"] = complete_input
                                        print(f"\nâœ… è§£æå·¥å…·å‚æ•°æˆåŠŸ: {complete_input}")
                                except json.JSONDecodeError as e:
                                    print(f"\nâš ï¸  å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯
                                    try:
                                        # å°è¯•æ·»åŠ å¼•å·å’Œå¤§æ‹¬å·
                                        fixed_json = '{' + current_tool_inputs[tool_id].replace(':', '":').replace('{', '{"').replace(',', ',"') + '}'
                                        fixed_json = fixed_json.replace(':"', ':"').replace('",', '",').replace('{"', '{"')
                                        complete_input = json.loads(fixed_json)
                                        assistant_response["content"][-1]["input"] = complete_input
                                        print(f"\nğŸ”§ ä¿®å¤åè§£ææˆåŠŸ: {complete_input}")
                                    except:
                                        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä½¿ç”¨ç©ºå¯¹è±¡
                                        assistant_response["content"][-1]["input"] = {}
                                        print(f"\nâŒ æ— æ³•ä¿®å¤JSONï¼Œä½¿ç”¨ç©ºå¯¹è±¡")
                    elif chunk.type == "message_stop":
                        break
                
                print(" âœ…")
                
                # æ”¶é›†å®Œæ•´çš„AIå“åº”æ–‡æœ¬
                full_ai_response.append(current_text_response)
                
                # æ·»åŠ å“åº”åˆ°æ¶ˆæ¯å†å²
                messages.append(assistant_response)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                has_tool_use = any(block.get("type") == "tool_use" for block in assistant_response["content"])
                
                if has_tool_use:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_results = []
                    
                    for content_block in assistant_response["content"]:
                        if content_block.get("type") == "tool_use":
                            tool_name = content_block["name"]
                            tool_input = content_block.get("input", {})
                            tool_id = content_block["id"]
                            
                            print(f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")
                            
                            # ç¡®ä¿å·¥å…·è¾“å…¥æ˜¯æœ‰æ•ˆçš„
                            if tool_name == "query_database":
                                if not tool_input or "sql" not in tool_input:
                                    # å¦‚æœæ²¡æœ‰SQLå‚æ•°ï¼Œå°è¯•ä»åç§°ä¸­æå–
                                    print(f"âš ï¸ ç¼ºå°‘SQLå‚æ•°ï¼Œå°è¯•ä¿®å¤...")
                                    sql = "SELECT * FROM " + self.current_table_name + " LIMIT 5"
                                    tool_input = {"sql": sql}
                                    print(f"ğŸ“ ä½¿ç”¨é»˜è®¤SQL: {sql}")
                                else:
                                    print(f"ğŸ“ SQL: {tool_input['sql']}")
                            
                            # æ‰§è¡Œå·¥å…·
                            result = self.execute_tool(tool_name, tool_input)
                            
                            # è®°å½•å®Œæ•´çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
                            tool_calls_made.append({
                                "tool_name": tool_name,
                                "tool_input": tool_input,
                                "result": result
                            })
                            
                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": tool_id,
                                "content": json.dumps(result, ensure_ascii=False, indent=2)
                            })
                    
                    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
                    messages.append({
                        "role": "user",
                        "content": tool_results
                    })
                    
                else:
                    # Claudeç»™å‡ºäº†æœ€ç»ˆå›ç­”ï¼Œä¿å­˜å®Œæ•´è®°å¿†
                    complete_ai_response = "\n".join(full_ai_response)
                    
                    # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šä¿å­˜å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬HTMLå†…å®¹å’Œå®Œæ•´SQLè®°å½•
                    self.memory.save_context(
                        user_input=user_query,
                        ai_response=complete_ai_response,
                        html_content=html_content,
                        tool_calls=tool_calls_made,  # åŒ…å«å®Œæ•´SQLæ‰§è¡Œç»“æœ
                        analysis_metadata={
                            "database": self.current_db_path,
                            "table": self.current_table_name,
                            "analysis_type": analysis_type,
                            "iterations": iteration,
                            "html_file": html_filename
                        }
                    )
                    
                    print(f"\nâœ… HTMLæŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
                    print(f"ğŸ“„ æ–‡ä»¶ä½ç½®: {os.path.abspath(html_filename)}")
                    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(html_filename):,} å­—èŠ‚")
                    print(f"ğŸ”§ å·¥å…·è°ƒç”¨æ¬¡æ•°: {len(tool_calls_made)}")
                    print(f"ğŸ’¾ å®Œæ•´è®°å½•å·²ä¿å­˜: HTML({len(html_content):,}å­—ç¬¦) + SQLè¯¦æƒ…")
                    
                    # å°è¯•è‡ªåŠ¨æ‰“å¼€HTMLæ–‡ä»¶
                    try:
                        import webbrowser
                        webbrowser.open(f'file://{os.path.abspath(html_filename)}')
                        print("ğŸŒ å·²è‡ªåŠ¨åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š")
                    except:
                        print("ğŸ’¡ è¯·æ‰‹åŠ¨æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹æŠ¥å‘Š")
                    
                    return f"HTMLæŠ¥å‘Šç”Ÿæˆå®Œæˆ!\næ–‡ä»¶: {html_filename}\nç±»å‹: {analysis_type}\nå·¥å…·è°ƒç”¨: {len(tool_calls_made)}æ¬¡\nå®Œæ•´è®°å½•å·²ä¿å­˜: HTML({len(html_content):,}å­—ç¬¦) + SQLè¯¦æƒ…"
                    
            except Exception as e:
                self.debug_print(f"APIè°ƒç”¨å¤±è´¥: {str(e)}", "ERROR")
                return f"åˆ†æå¤±è´¥: {str(e)}"
        
        return f"åˆ†æè¿›è¡Œäº†{iteration}è½®å¯¹è¯åè¾¾åˆ°é™åˆ¶"
    
    def show_memory_status(self):
        """æ˜¾ç¤ºè®°å¿†çŠ¶æ€"""
        summary = self.memory.get_memory_summary()
        print("\nğŸ“š å½“å‰è®°å¿†çŠ¶æ€:")
        print(f"  ğŸ“Š å¯¹è¯è®°å½•æ•°: {summary['conversation_count']}")
        if summary.get('latest_timestamp'):
            print(f"  â° æœ€æ–°è®°å½•æ—¶é—´: {summary['latest_timestamp'][:19]}")
        
        if summary['conversation_count'] > 0:
            print(f"  â“ æœ€æ–°é—®é¢˜: {summary['latest_query'][:60]}...")
            print(f"  ğŸ—ƒï¸  æ€»SQLè°ƒç”¨: {summary.get('total_sql_calls', 0)}æ¬¡")
            print(f"  ğŸ“„ åŒ…å«HTML: {'âœ… ' + str(summary.get('latest_html_size', 0)) + 'å­—ç¬¦' if summary.get('has_html') else 'âŒ'}")
    
    def show_sql_history(self):
        """æ˜¾ç¤ºSQLæ‰§è¡Œå†å²"""
        self.memory.show_sql_details()
    
    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        self.memory.clear_memory()


def select_csv_file():
    """é€‰æ‹©CSVæ–‡ä»¶"""
    root = tk.Tk()
    root.withdraw()
    root.attributes('-topmost', True)
    
    try:
        csv_file = filedialog.askopenfilename(
            title="è¯·é€‰æ‹©CSVæ–‡ä»¶",
            filetypes=[("CSVæ–‡ä»¶", "*.csv"), ("æ‰€æœ‰æ–‡ä»¶", "*.*")],
            initialdir=os.getcwd()
        )
        root.destroy()
        return csv_file if csv_file else None
    except Exception as e:
        root.destroy()
        print(f"æ–‡ä»¶é€‰æ‹©å¤±è´¥: {e}")
        return None

def main():
    """ä¸»ç¨‹åº - å®Œæ•´SQLè®°å½•ç‰ˆæœ¬"""
    print("ğŸ¤– æ™ºèƒ½æ•°æ®åº“åˆ†æç³»ç»Ÿ (å®Œæ•´SQLè®°å½•ç‰ˆ v2.1)")
    print("=" * 60)
    
    # é…ç½®APIå¯†é’¥
    api_key = os.getenv('ANTHROPIC_API_KEY')
    
    if not api_key:
        print("æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY")
        api_key = input("è¯·è¾“å…¥æ‚¨çš„ Anthropic API å¯†é’¥: ").strip()
    
    if not api_key:
        print("âŒ æœªæä¾›APIå¯†é’¥ï¼Œç¨‹åºæ— æ³•ç»§ç»­")
        return
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = DatabaseAnalyzer(api_key)
    
    # è°ƒè¯•æ¨¡å¼è®¾ç½®
    debug_choice = input("æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Ÿ(y/n, é»˜è®¤n): ").strip().lower()
    analyzer.set_debug_mode(debug_choice == 'y')
    
    # æ˜¾ç¤ºå½“å‰è®°å¿†çŠ¶æ€
    analyzer.show_memory_status()
    
    # æ•°æ®å¯¼å…¥é€‰æ‹©
    print("\nğŸ“ æ•°æ®ç®¡ç†é€‰é¡¹:")
    print("1. å¯¼å…¥æ–°çš„CSVæ–‡ä»¶")
    print("2. ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æ•°æ®")
    print("3. æŸ¥çœ‹è®°å¿†çŠ¶æ€")
    print("4. æŸ¥çœ‹å¯¹è¯åˆ—è¡¨")
    print("5. æŸ¥çœ‹SQLæ‰§è¡Œå†å²")
    print("6. æ¸…ç©ºè®°å¿†")
    
    choice = input("è¯·é€‰æ‹©æ“ä½œ (1-6, é»˜è®¤1): ").strip() or "1"
    
    if choice == "1":
        csv_file = select_csv_file()
        if not csv_file:
            print("æœªé€‰æ‹©æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
            return
        
        table_name = input("è¯·è¾“å…¥è¡¨å (é»˜è®¤: data_table): ").strip() or "data_table"
        db_path = input("è¯·è¾“å…¥æ•°æ®åº“è·¯å¾„ (é»˜è®¤: analysis.db): ").strip() or "analysis.db"
        
        print(f"\næ­£åœ¨å¯¼å…¥æ•°æ®...")
        import_result = analyzer.import_csv_to_sqlite(csv_file, table_name, db_path)
        
        if not import_result["success"]:
            print(f"âŒ {import_result['message']}")
            return
        
        print(f"âœ… {import_result['message']}")
        
    elif choice == "2":
        if not analyzer.current_db_path:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¹‹å‰çš„æ•°æ®åº“ï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®")
            return
        print(f"âœ… ç»§ç»­ä½¿ç”¨æ•°æ®åº“: {analyzer.current_db_path}")
        
    elif choice == "3":
        analyzer.show_memory_status()
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        
    elif choice == "4":
        analyzer.memory.show_conversation_list()
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        
    elif choice == "5":
        analyzer.show_sql_history()
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        
    elif choice == "6":
        analyzer.clear_memory()
        input("æŒ‰å›è½¦é”®ç»§ç»­...")
    
    # æ™ºèƒ½åˆ†æäº¤äº’
    print(f"\nğŸ¤– æ™ºèƒ½åˆ†ææ¨¡å¼å·²å¯åŠ¨ï¼(å®Œæ•´SQLè®°å½•ç‰ˆ)")
    print("ğŸ’¡ ç°åœ¨ä¼šä¿å­˜æ¯ä¸ªSQLçš„å®Œæ•´æ‰§è¡Œç»“æœå’ŒHTMLå†…å®¹")
    print("\nğŸ“‹ å¯ç”¨å‘½ä»¤:")
    print("  - 'memory': æŸ¥çœ‹è®°å¿†çŠ¶æ€")
    print("  - 'list': æŸ¥çœ‹å¯¹è¯åˆ—è¡¨")
    print("  - 'sql': æŸ¥çœ‹å®Œæ•´SQLæ‰§è¡Œå†å²")
    print("  - 'sql [ID]': æŸ¥çœ‹æŒ‡å®šå¯¹è¯çš„SQLå†å²")
    print("  - 'export [ID]': å¯¼å‡ºæŒ‡å®šå¯¹è¯çš„HTML")
    print("  - 'html [ID]': æŸ¥çœ‹æŒ‡å®šå¯¹è¯çš„HTMLå†…å®¹")
    print("  - 'clear': æ¸…ç©ºè®°å¿†")
    print("  - 'quit': é€€å‡ºç¨‹åº")
    print("\nğŸš€ ç¤ºä¾‹é—®é¢˜ï¼š")
    print("  - 'å¸®æˆ‘åˆ†æè¿™ä¸ªæ•°æ®é›†çš„åŸºæœ¬æƒ…å†µ'")
    print("  - 'ç»™æˆ‘ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„HTMLåˆ†ææŠ¥å‘Š'")
    print("  - 'åŸºäºå¯¹è¯1çš„åˆ†æï¼Œé‡æ–°ç”Ÿæˆæ›´è¯¦ç»†çš„æŠ¥å‘Š'")
    print("  - 'ä½¿ç”¨å¯¹è¯2çš„SQLæŸ¥è¯¢ç»“æœï¼Œç”Ÿæˆä¸åŒé£æ ¼çš„å›¾è¡¨'")
    print("-" * 60)
    
    while True:
        try:
            query = input("\nğŸ” è¯·æè¿°æ‚¨çš„åˆ†æéœ€æ±‚: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼æ‰€æœ‰è®°å¿†å·²è‡ªåŠ¨ä¿å­˜ã€‚")
                break
                
            elif query.lower() in ['memory', 'è®°å¿†']:
                analyzer.show_memory_status()
                continue
                
            elif query.lower() in ['list', 'åˆ—è¡¨']:
                analyzer.memory.show_conversation_list()
                continue
                
            elif query.lower().startswith('sql'):
                parts = query.split()
                if len(parts) > 1 and parts[1].isdigit():
                    analyzer.memory.show_sql_details(int(parts[1]))
                else:
                    analyzer.memory.show_sql_details()
                continue
                
            elif query.lower().startswith('export'):
                parts = query.split()
                if len(parts) > 1 and parts[1].isdigit():
                    analyzer.memory.export_html_report(int(parts[1]))
                else:
                    print("ç”¨æ³•: export [å¯¹è¯ID]")
                continue
                
            elif query.lower().startswith('html'):
                parts = query.split()
                if len(parts) > 1 and parts[1].isdigit():
                    html_content = analyzer.memory.get_html_content(int(parts[1]))
                    if html_content:
                        print(f"ğŸ“„ HTMLå†…å®¹é•¿åº¦: {len(html_content):,} å­—ç¬¦")
                        print(f"ğŸ“‹ é¢„è§ˆ: {html_content[:300]}...")
                        
                        export_choice = input("æ˜¯å¦å¯¼å‡ºåˆ°æ–°æ–‡ä»¶ï¼Ÿ(y/n): ").strip().lower()
                        if export_choice == 'y':
                            analyzer.memory.export_html_report(int(parts[1]))
                    else:
                        print("âŒ æŒ‡å®šå¯¹è¯æ²¡æœ‰HTMLå†…å®¹")
                else:
                    print("ç”¨æ³•: html [å¯¹è¯ID]")
                continue
                
            elif query.lower() in ['clear', 'æ¸…ç©º']:
                confirm = input("âš ï¸  ç¡®è®¤æ¸…ç©ºæ‰€æœ‰è®°å¿†ï¼Ÿè¿™å°†åˆ›å»ºå¤‡ä»½æ–‡ä»¶ã€‚(y/n): ").strip().lower()
                if confirm == 'y':
                    analyzer.clear_memory()
                continue
            
            if not query:
                continue
            
            print(f"\nğŸ”„ Claudeæ­£åœ¨æ™ºèƒ½åˆ†æä¸­...")
            
            # æ‰§è¡Œæ™ºèƒ½åˆ†æ
            result = analyzer.analyze_with_llm(query)
            
            print("\n" + "="*80)
            print("ğŸ¤– æ™ºèƒ½åˆ†æç»“æœ:")
            print("="*80)
            print(result)
            print("="*80)
            
            # æ˜¾ç¤ºæœ¬æ¬¡åˆ†æçš„ç»Ÿè®¡
            summary = analyzer.memory.get_memory_summary()
            
            print(f"\nğŸ“Š å½“å‰è®°å¿†çŠ¶æ€:")
            print(f"  ğŸ“ˆ æ€»å¯¹è¯æ•°: {summary['conversation_count']}")
            print(f"  ğŸ—ƒï¸  æ€»SQLè°ƒç”¨: {summary.get('total_sql_calls', 0)}")
            print(f"  ğŸ“„ æœ€æ–°HTML: {'âœ… ' + str(summary.get('latest_html_size', 0)) + 'å­—ç¬¦' if summary.get('has_html') else 'âŒ'}")
            print(f"  ğŸ’¾ è®°å¿†æ–‡ä»¶: {summary.get('memory_file', 'N/A')}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œæ‰€æœ‰è®°å¿†å·²ä¿å­˜ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
            if analyzer.debug_mode:
                import traceback
                traceback.print_exc()

if __name__ == "__main__":
    main()