# P1ç²¾ç®€ç‰ˆæ•°æ®åº“åˆ†æå™¨ - æ¸…ç†è°ƒè¯•è¾“å‡º
# ç‰ˆæœ¬: 3.1.0 - P1é˜¶æ®µç²¾ç®€ç‰ˆ - å¤šè¡¨æ”¯æŒ

from anthropic import Anthropic
import sqlite3
import pandas as pd
import os
from datetime import datetime
import json
import re
from typing import Dict, List, Optional, Any
try:
    from .data_processor import DataProcessor
except ImportError:
    from data_processor import DataProcessor

class DatabaseAnalyzer:
    """P1ç²¾ç®€ç‰ˆæ•°æ®åº“åˆ†æå™¨ç±» - ä¸“æ³¨æ ¸å¿ƒæ•°æ®å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒå¤šè¡¨ç®¡ç†"""
    
    def __init__(self, api_key, model_name="claude-sonnet-4-20250514", base_url=None):
        """
        åˆå§‹åŒ–æ•°æ®åº“åˆ†æå™¨
        
        Args:
            api_key: APIå¯†é’¥
            model_name: æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
        """
        client_params = {"api_key": api_key}
        if base_url:
            client_params["base_url"] = base_url
        
        self.client = Anthropic(**client_params)
        self.model_name = model_name
        self.current_db_path = None
        self.current_table_name = None  # ä¿æŒå…¼å®¹æ€§
        self.conversation_tables = []  # æ–°å¢ï¼šå½“å‰å¯¹è¯ä¸­çš„æ‰€æœ‰è¡¨
        self.data_processor = DataProcessor()  # æ–°å¢ï¼šæ•°æ®å¤„ç†å™¨
        
        # å®šä¹‰å·¥å…· - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
        self.tools = [
            {
                "name": "query_database",
                "description": "æ‰§è¡ŒSQLæŸ¥è¯¢è·å–æ•°æ®åº“ä¿¡æ¯ï¼Œæ”¯æŒå¤šè¡¨æŸ¥è¯¢",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "sql": {
                            "type": "string",
                            "description": "è¦æ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥ï¼Œå¯ä»¥æŸ¥è¯¢å¤šä¸ªè¡¨"
                        }
                    },
                    "required": ["sql"]
                }
            },
            {
                "name": "get_table_info",
                "description": "è·å–å½“å‰å¯¹è¯ä¸­æ‰€æœ‰è¡¨çš„ç»“æ„ä¿¡æ¯å’Œæ ·æœ¬æ•°æ®",
                "input_schema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            }
        ]
    
    def _generate_table_name(self, filename: str) -> str:
        """
        åŸºäºæ–‡ä»¶åç”Ÿæˆå”¯ä¸€çš„è¡¨å
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            æ¸…ç†åçš„è¡¨å
        """
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        base_name = os.path.splitext(filename)[0]
        
        # æ¸…ç†æ–‡ä»¶åï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å’Œä¸‹åˆ’çº¿
        cleaned_name = re.sub(r'[^\w\u4e00-\u9fff]', '_', base_name)
        cleaned_name = re.sub(r'_+', '_', cleaned_name)
        cleaned_name = cleaned_name.strip('_')
        
        # å¦‚æœåç§°ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨é»˜è®¤åç§°
        if not cleaned_name or len(cleaned_name) < 2:
            cleaned_name = "data_table"
        
        # æ·»åŠ æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        table_name = f"{cleaned_name}_{timestamp}"
        
        # ç¡®ä¿è¡¨åä¸è¶…è¿‡SQLiteé™åˆ¶ï¼ˆé€šå¸¸ä¸º64å­—ç¬¦ï¼‰
        if len(table_name) > 60:
            table_name = f"{cleaned_name[:30]}_{timestamp}"
        
        return table_name
    
    def add_table_to_conversation(self, table_name: str, filename: str, columns: List[str], row_count: int):
        """
        å°†æ–°è¡¨æ·»åŠ åˆ°å½“å‰å¯¹è¯çš„è¡¨åˆ—è¡¨ä¸­
        
        Args:
            table_name: è¡¨å
            filename: åŸå§‹æ–‡ä»¶å
            columns: åˆ—ååˆ—è¡¨
            row_count: è¡Œæ•°
        """
        table_info = {
            "table_name": table_name,
            "original_filename": filename,
            "columns": columns,
            "row_count": row_count,
            "created_at": datetime.now().isoformat(),
            "description": f"ä»æ–‡ä»¶ {filename} å¯¼å…¥çš„æ•°æ®è¡¨"
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåè¡¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°
        existing_index = None
        for i, table in enumerate(self.conversation_tables):
            if table["table_name"] == table_name:
                existing_index = i
                break
        
        if existing_index is not None:
            self.conversation_tables[existing_index] = table_info
            print(f"ğŸ“‹ æ›´æ–°è¡¨ä¿¡æ¯: {table_name}")
        else:
            self.conversation_tables.append(table_info)
            print(f"ğŸ“‹ æ–°å¢è¡¨ä¿¡æ¯: {table_name}")
        
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œè®¾ç½®current_table_nameä¸ºæœ€æ–°çš„è¡¨
        self.current_table_name = table_name
        
        print(f"ğŸ“Š å½“å‰å¯¹è¯å…±æœ‰ {len(self.conversation_tables)} ä¸ªæ•°æ®è¡¨")
        
    def get_conversation_tables_summary(self) -> str:
        """
        è·å–å½“å‰å¯¹è¯ä¸­æ‰€æœ‰è¡¨çš„æ‘˜è¦ä¿¡æ¯
        
        Returns:
            è¡¨æ‘˜è¦å­—ç¬¦ä¸²
        """
        if not self.conversation_tables:
            return "å½“å‰å¯¹è¯ä¸­æ²¡æœ‰æ•°æ®è¡¨"
        
        summary = f"å½“å‰å¯¹è¯ä¸­å…±æœ‰ {len(self.conversation_tables)} ä¸ªæ•°æ®è¡¨ï¼š\n"
        for i, table in enumerate(self.conversation_tables, 1):
            summary += f"{i}. è¡¨å: {table['table_name']}\n"
            summary += f"   æ¥æºæ–‡ä»¶: {table['original_filename']}\n"
            summary += f"   åˆ—æ•°: {len(table['columns'])}\n"
            summary += f"   è¡Œæ•°: {table['row_count']}\n"
            summary += f"   åˆ›å»ºæ—¶é—´: {table['created_at'][:19]}\n\n"
        
        return summary
        
    def import_file_to_sqlite(self, file_path, table_name, db_path="analysis_db.db", processing_options=None):
        """ä»å¤šç§æ ¼å¼æ–‡ä»¶åˆ›å»ºSQLiteè¡¨å¹¶å¯¼å…¥æ•°æ® - æ”¯æŒå¤šè¡¨å…±å­˜å’Œæ•°æ®å¤„ç†"""
        try:
            print(f"ğŸ“¥ å¼€å§‹å¯¼å…¥æ–‡ä»¶: {file_path}")
            print(f"ğŸ“Š ç›®æ ‡æ•°æ®åº“: {db_path}")
            print(f"ğŸ“‹ ç›®æ ‡è¡¨å: {table_name}")
            
            if not os.path.exists(file_path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return {"success": False, "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
            
            # æ£€æµ‹æ–‡ä»¶æ ¼å¼
            try:
                file_format = self.data_processor.detect_file_format(file_path)
                print(f"ğŸ“‹ æ£€æµ‹åˆ°æ–‡ä»¶æ ¼å¼: {file_format}")
            except ValueError as e:
                print(f"âŒ {str(e)}")
                return {"success": False, "message": str(e)}
            
            # è¯»å–æ–‡ä»¶
            print("ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶...")
            try:
                df = self.data_processor.read_file(file_path)
                print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
            except Exception as e:
                print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
                return {"success": False, "message": f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"}
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡è¯„ä¼°...")
            quality_report = self.data_processor.assess_data_quality(df)
            
            # æ•°æ®æ¸…æ´—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            cleaning_log = None
            if processing_options is None:
                processing_options = {"enable_cleaning": True}
            
            if processing_options.get("enable_cleaning", True):
                print("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
                cleaning_options = processing_options.get("cleaning_options", {})
                df, cleaning_log = self.data_processor.clean_data(df, cleaning_options)
                print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
            
            # è¿æ¥åˆ°SQLiteæ•°æ®åº“
            print(f"ğŸ”Œ æ­£åœ¨è¿æ¥æ•°æ®åº“: {db_path}")
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table_name,))
            table_exists = cursor.fetchone() is not None
            
            if table_exists:
                print(f"ğŸ”„ è¡¨ {table_name} å·²å­˜åœ¨ï¼Œå°†æ›¿æ¢æ•°æ®...")
                cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
            else:
                print(f"ğŸ†• åˆ›å»ºæ–°è¡¨: {table_name}")
            
            # åˆ›å»ºæ–°è¡¨å¹¶å¯¼å…¥æ•°æ®
            print("ğŸ“ æ­£åœ¨å¯¼å…¥æ•°æ®...")
            df.to_sql(table_name, conn, if_exists='replace', index=False)
            
            # è·å–å¯¼å…¥çš„è¡Œæ•°
            print("ğŸ”¢ æ­£åœ¨ç»Ÿè®¡å¯¼å…¥è¡Œæ•°...")
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            rows_count = cursor.fetchone()[0]
            
            conn.commit()
            conn.close()
            
            # ä¿å­˜å½“å‰æ•°æ®åº“ä¿¡æ¯
            self.current_db_path = db_path
            
            # è·å–åŸå§‹æ–‡ä»¶å
            original_filename = os.path.basename(file_path)
            
            # æ·»åŠ åˆ°å¯¹è¯è¡¨åˆ—è¡¨
            self.add_table_to_conversation(table_name, original_filename, list(df.columns), rows_count)
            
            print(f"âœ… å¯¼å…¥å®Œæˆï¼Œå…±å¯¼å…¥ {rows_count} è¡Œæ•°æ®")
            
            # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            processing_report = None
            if cleaning_log:
                processing_report = self.data_processor.generate_processing_report(quality_report, cleaning_log)
            
            return {
                "success": True,
                "message": f"æˆåŠŸå¯¼å…¥ {rows_count} è¡Œæ•°æ®åˆ°è¡¨ '{table_name}'",
                "rows_imported": rows_count,
                "columns": list(df.columns),
                "table_name": table_name,
                "total_tables": len(self.conversation_tables),
                "file_format": file_format,
                "quality_report": quality_report,
                "cleaning_log": cleaning_log,
                "processing_report": processing_report
            }
            
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {str(e)}")
            return {"success": False, "message": f"å¯¼å…¥å¤±è´¥: {str(e)}"}
    
    def import_csv_to_sqlite(self, csv_file_path, table_name, db_path="analysis_db.db"):
        """ä¿æŒå‘åå…¼å®¹æ€§çš„CSVå¯¼å…¥æ–¹æ³•"""
        return self.import_file_to_sqlite(csv_file_path, table_name, db_path)
    
    def _clean_column_name(self, col_name):
        """æ¸…ç†åˆ—å"""
        cleaned = str(col_name).strip()
        cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '_', cleaned)
        cleaned = re.sub(r'_+', '_', cleaned)
        cleaned = cleaned.strip('_')
        return cleaned or 'unnamed_column'
    
    def get_table_schema(self):
        """è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„ç»“æ„ä¿¡æ¯"""
        if not self.current_db_path:
            return "æœªè¿æ¥åˆ°æ•°æ®åº“"
        
        try:
            conn = sqlite3.connect(self.current_db_path)
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰ç”¨æˆ·æ•°æ®è¡¨ï¼ˆæ’é™¤ç³»ç»Ÿè¡¨ï¼‰
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name != '_db_info'
                ORDER BY name
            """)
            tables = cursor.fetchall()
            
            if not tables:
                conn.close()
                return "æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®è¡¨"
            
            all_tables_info = []
            
            for table_row in tables:
                table_name = table_row[0]
                
                # è·å–è¡¨ç»“æ„
                schema_info = cursor.execute(f"PRAGMA table_info({table_name})").fetchall()
                
                # è·å–æ ·æœ¬æ•°æ®
                sample_data = cursor.execute(f"SELECT * FROM {table_name} LIMIT 3").fetchall()
                column_names = [description[0] for description in cursor.description]
                
                # è·å–è¡Œæ•°
                row_count = cursor.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
                
                # ä»conversation_tablesä¸­è·å–æ›´å¤šä¿¡æ¯
                table_meta = None
                for table_info in self.conversation_tables:
                    if table_info["table_name"] == table_name:
                        table_meta = table_info
                        break
                
                table_schema = {
                    "table_name": table_name,
                    "original_filename": table_meta["original_filename"] if table_meta else "æœªçŸ¥",
                    "description": table_meta["description"] if table_meta else f"æ•°æ®è¡¨ {table_name}",
                    "columns": [{"name": col[1], "type": col[2]} for col in schema_info],
                    "row_count": row_count,
                    "sample_data": [dict(zip(column_names, row)) for row in sample_data],
                    "created_at": table_meta["created_at"] if table_meta else "æœªçŸ¥"
                }
                
                all_tables_info.append(table_schema)
            
            conn.close()
            
            # æ„å»ºç»¼åˆä¿¡æ¯
            result = {
                "database_path": self.current_db_path,
                "total_tables": len(all_tables_info),
                "tables": all_tables_info,
                "summary": self.get_conversation_tables_summary()
            }
            
            return result
            
        except Exception as e:
            return f"è·å–è¡¨ç»“æ„å¤±è´¥: {str(e)}"
    
    def clear_conversation_tables(self):
        """æ¸…ç©ºå½“å‰å¯¹è¯çš„è¡¨åˆ—è¡¨ï¼ˆæ–°å¯¹è¯æ—¶è°ƒç”¨ï¼‰"""
        self.conversation_tables = []
        self.current_table_name = None
        print("ğŸ§¹ å·²æ¸…ç©ºå¯¹è¯è¡¨åˆ—è¡¨")
    
    def _sync_tables_from_database(self):
        """
        ä»æ•°æ®åº“ä¸­åŒæ­¥è¡¨åˆ—è¡¨åˆ°conversation_tables
        ç”¨äºåˆ‡æ¢å¯¹è¯æ—¶é‡æ–°åŠ è½½è¡¨ä¿¡æ¯
        """
        if not self.current_db_path:
            return
        
        try:
            conn = sqlite3.connect(self.current_db_path)
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰ç”¨æˆ·æ•°æ®è¡¨
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%' AND name != '_db_info'
                ORDER BY name
            """)
            tables = cursor.fetchall()
            
            # æ¸…ç©ºç°æœ‰åˆ—è¡¨
            self.conversation_tables = []
            
            for table_row in tables:
                table_name = table_row[0]
                
                # è·å–è¡¨ä¿¡æ¯
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns_info = cursor.fetchall()
                columns = [col[1] for col in columns_info]
                
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                row_count = cursor.fetchone()[0]
                
                # å°è¯•ä»è¡¨åæ¨æ–­åŸå§‹æ–‡ä»¶å
                original_filename = "æœªçŸ¥æ–‡ä»¶"
                if "_" in table_name:
                    # ç§»é™¤æ—¶é—´æˆ³éƒ¨åˆ†
                    parts = table_name.split("_")
                    if len(parts) >= 2:
                        # å‡è®¾æœ€åä¸€ä¸ªæˆ–ä¸¤ä¸ªéƒ¨åˆ†æ˜¯æ—¶é—´æˆ³
                        name_parts = parts[:-1] if len(parts[-1]) == 6 else parts[:-2]
                        original_filename = "_".join(name_parts) + ".csv"
                
                table_info = {
                    "table_name": table_name,
                    "original_filename": original_filename,
                    "columns": columns,
                    "row_count": row_count,
                    "created_at": "æœªçŸ¥",  # åˆ‡æ¢å¯¹è¯æ—¶æ— æ³•è·å–å‡†ç¡®çš„åˆ›å»ºæ—¶é—´
                    "description": f"æ•°æ®è¡¨ {table_name}"
                }
                
                self.conversation_tables.append(table_info)
            
            conn.close()
            
            # è®¾ç½®current_table_nameä¸ºæœ€æ–°çš„è¡¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if self.conversation_tables:
                self.current_table_name = self.conversation_tables[-1]["table_name"]
            
            print(f"ğŸ”„ å·²åŒæ­¥ {len(self.conversation_tables)} ä¸ªè¡¨åˆ°å¯¹è¯åˆ—è¡¨")
            
        except Exception as e:
            print(f"âš ï¸ åŒæ­¥è¡¨åˆ—è¡¨å¤±è´¥: {e}")
            self.conversation_tables = []
        
    def query_database(self, sql):
        """æ‰§è¡ŒSQLæŸ¥è¯¢ - æ”¯æŒå¤šè¡¨æŸ¥è¯¢"""
        if not self.current_db_path:
            return {"error": "æœªè¿æ¥åˆ°æ•°æ®åº“"}
        
        try:
            conn = sqlite3.connect(self.current_db_path)
            cursor = conn.cursor()
            
            start_time = datetime.now()
            cursor.execute(sql)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            if sql.strip().upper().startswith('SELECT'):
                results = cursor.fetchall()
                columns = [description[0] for description in cursor.description]
                
                result_data = {
                    "columns": columns,
                    "data": results,
                    "row_count": len(results),
                    "sql_executed": sql,
                    "execution_time": execution_time,
                    "query_type": "SELECT",
                    "data_preview": results[:5] if results else [],
                    "available_tables": [table["table_name"] for table in self.conversation_tables]
                }
                
                conn.close()
                return result_data
            else:
                conn.commit()
                conn.close()
                return {
                    "message": "æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ", 
                    "sql_executed": sql, 
                    "execution_time": execution_time,
                    "query_type": "NON_SELECT",
                    "available_tables": [table["table_name"] for table in self.conversation_tables]
                }
                
        except Exception as e:
            return {
                "error": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}", 
                "sql_attempted": sql,
                "execution_time": 0,
                "query_type": "ERROR",
                "available_tables": [table["table_name"] for table in self.conversation_tables]
            }
    
    def execute_tool(self, tool_name, tool_input):
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        try:
            if tool_name == "query_database":
                sql = tool_input.get("sql", "")
                if not sql:
                    return {"error": "SQLå‚æ•°ä¸ºç©º", "query_type": "ERROR"}
                # å±é™©SQLæ£€æµ‹ï¼ˆä¿®æ­£ç‰ˆï¼‰
                sql_lower = sql.lower().replace('\n', ' ').strip()
                # æ£€æŸ¥SELECT * FROM
                if re.search(r"select\s+\*\s+from", sql_lower):
                    # å¦‚æœæ²¡æœ‰LIMITåˆ™å±é™©
                    if "limit" not in sql_lower:
                        return {
                            "error": f"âš ï¸ æ£€æµ‹åˆ°ä½ çš„SQLå‘½ä»¤ä¸º: {sql}\nè¯¥å‘½ä»¤ä¼šè¿”å›å¤§é‡æ•°æ®ï¼Œææ˜“å¯¼è‡´tokenè¶…é™å’Œç³»ç»Ÿå´©æºƒã€‚è¯·æ”¹ç”¨ç»Ÿè®¡ã€é‡‡æ ·æˆ–åŠ LIMITçš„æ–¹å¼æŸ¥è¯¢ã€‚",
                            "query_type": "DANGEROUS_SQL"
                        }
                    # æœ‰LIMITä½†limitå€¼è¿‡å¤§ä¹Ÿå±é™©
                    m = re.search(r"limit\s+(\d+)", sql_lower)
                    if m and int(m.group(1)) > 100:
                        return {
                            "error": f"âš ï¸ æ£€æµ‹åˆ°ä½ çš„SQLå‘½ä»¤ä¸º: {sql}\nLIMITå€¼è¿‡å¤§ï¼Œææ˜“å¯¼è‡´tokenè¶…é™å’Œç³»ç»Ÿå´©æºƒã€‚å»ºè®®LIMITä¸è¶…è¿‡100ã€‚",
                            "query_type": "DANGEROUS_SQL"
                        }
                # å…¶å®ƒå±é™©æ¨¡å¼
                dangerous_patterns = [
                    r"into\s+outfile",  # å¯¼å‡º
                    r"copy.+to",  # COPY TO
                    r"union",  # UNION
                ]
                for pattern in dangerous_patterns:
                    if re.search(pattern, sql_lower):
                        return {
                            "error": f"âš ï¸ æ£€æµ‹åˆ°ä½ çš„SQLå‘½ä»¤ä¸º: {sql}\nè¯¥å‘½ä»¤å­˜åœ¨é«˜é£é™©æ“ä½œï¼Œå·²è¢«æ‹¦æˆªã€‚",
                            "query_type": "DANGEROUS_SQL"
                        }
                return self.query_database(sql)
            
            elif tool_name == "get_table_info":
                result = self.get_table_schema()
                return {
                    "table_info": result,
                    "query_type": "TABLE_INFO",
                    "execution_time": 0.001
                }
            
            else:
                return {"error": f"æœªçŸ¥å·¥å…·: {tool_name}", "query_type": "ERROR"}
        except Exception as e:
            return {
                "error": f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}",
                "query_type": "ERROR",
                "execution_time": 0
            }