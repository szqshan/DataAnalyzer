import pandas as pd
import numpy as np
import os
import re
import json
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

class DataProcessor:
    """æ•°æ®å¤„ç†å’Œæ¸…æ´—æ¨¡å—"""
    
    def __init__(self):
        self.supported_formats = ['.csv', '.xlsx', '.xls', '.json', '.tsv', '.txt']
        self.quality_report = {}
        self.cleaning_log = []
        
    def detect_file_format(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        _, ext = os.path.splitext(file_path.lower())
        if ext in self.supported_formats:
            return ext
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(self.supported_formats)}")
    
    def read_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """æ™ºèƒ½è¯»å–å„ç§æ ¼å¼çš„æ–‡ä»¶"""
        try:
            file_format = self.detect_file_format(file_path)
            print(f"ğŸ“– æ£€æµ‹åˆ°æ–‡ä»¶æ ¼å¼: {file_format}")
            
            if file_format == '.csv':
                # å°è¯•å¤šç§ç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']
                for encoding in encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding, **kwargs)
                        print(f"âœ… ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸè¯»å–CSVæ–‡ä»¶")
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise ValueError("æ— æ³•ä½¿ç”¨å¸¸è§ç¼–ç è¯»å–CSVæ–‡ä»¶")
                    
            elif file_format in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path, **kwargs)
                print(f"âœ… æˆåŠŸè¯»å–Excelæ–‡ä»¶")
                
            elif file_format == '.json':
                df = pd.read_json(file_path, **kwargs)
                print(f"âœ… æˆåŠŸè¯»å–JSONæ–‡ä»¶")
                
            elif file_format == '.tsv':
                df = pd.read_csv(file_path, sep='\t', **kwargs)
                print(f"âœ… æˆåŠŸè¯»å–TSVæ–‡ä»¶")
                
            elif file_format == '.txt':
                # å°è¯•æ£€æµ‹åˆ†éš”ç¬¦
                with open(file_path, 'r', encoding='utf-8') as f:
                    first_line = f.readline()
                
                if '\t' in first_line:
                    df = pd.read_csv(file_path, sep='\t', **kwargs)
                elif '|' in first_line:
                    df = pd.read_csv(file_path, sep='|', **kwargs)
                else:
                    df = pd.read_csv(file_path, **kwargs)
                print(f"âœ… æˆåŠŸè¯»å–TXTæ–‡ä»¶")
            
            print(f"ğŸ“Š æ–‡ä»¶è¯»å–å®Œæˆ: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
            return df
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
            raise
    
    def assess_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡è¯„ä¼°...")
        
        quality_report = {
            "basic_info": {
                "rows": len(df),
                "columns": len(df.columns),
                "memory_usage": df.memory_usage(deep=True).sum(),
                "file_size_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
            },
            "column_analysis": {},
            "data_quality_issues": {
                "missing_values": {},
                "duplicate_rows": 0,
                "data_type_issues": [],
                "outliers": {},
                "inconsistent_formats": {}
            },
            "recommendations": [],
            "overall_score": 0
        }
        
        # åŸºç¡€ç»Ÿè®¡
        total_cells = len(df) * len(df.columns)
        missing_cells = df.isnull().sum().sum()
        
        # åˆ—åˆ†æ
        for col in df.columns:
            col_info = {
                "dtype": str(df[col].dtype),
                "non_null_count": df[col].count(),
                "null_count": df[col].isnull().sum(),
                "null_percentage": round(df[col].isnull().sum() / len(df) * 100, 2),
                "unique_count": df[col].nunique(),
                "unique_percentage": round(df[col].nunique() / len(df) * 100, 2)
            }
            
            # æ•°å€¼åˆ—ç»Ÿè®¡
            if df[col].dtype in ['int64', 'float64']:
                col_info.update({
                    "mean": df[col].mean(),
                    "std": df[col].std(),
                    "min": df[col].min(),
                    "max": df[col].max(),
                    "median": df[col].median()
                })
                
                # æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
                
                if len(outliers) > 0:
                    quality_report["data_quality_issues"]["outliers"][col] = {
                        "count": len(outliers),
                        "percentage": round(len(outliers) / len(df) * 100, 2),
                        "bounds": {"lower": lower_bound, "upper": upper_bound}
                    }
            
            # æ–‡æœ¬åˆ—åˆ†æ
            elif df[col].dtype == 'object':
                col_info.update({
                    "most_common": df[col].value_counts().head(3).to_dict() if len(df[col].dropna()) > 0 else {},
                    "avg_length": df[col].astype(str).str.len().mean() if len(df[col].dropna()) > 0 else 0
                })
                
                # æ£€æµ‹æ ¼å¼ä¸ä¸€è‡´
                if col.lower() in ['email', 'é‚®ç®±', 'phone', 'ç”µè¯', 'date', 'æ—¥æœŸ']:
                    inconsistent = self._check_format_consistency(df[col], col)
                    if inconsistent:
                        quality_report["data_quality_issues"]["inconsistent_formats"][col] = inconsistent
            
            quality_report["column_analysis"][col] = col_info
            
            # è®°å½•ç¼ºå¤±å€¼é—®é¢˜
            if col_info["null_percentage"] > 0:
                quality_report["data_quality_issues"]["missing_values"][col] = {
                    "count": col_info["null_count"],
                    "percentage": col_info["null_percentage"]
                }
        
        # é‡å¤è¡Œæ£€æµ‹
        duplicate_count = df.duplicated().sum()
        quality_report["data_quality_issues"]["duplicate_rows"] = {
            "count": duplicate_count,
            "percentage": round(duplicate_count / len(df) * 100, 2)
        }
        
        # æ•°æ®ç±»å‹é—®é¢˜æ£€æµ‹
        for col in df.columns:
            if df[col].dtype == 'object':
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¯æ•°å€¼ç±»å‹
                numeric_convertible = pd.to_numeric(df[col], errors='coerce').notna().sum()
                if numeric_convertible > len(df) * 0.8:  # 80%ä»¥ä¸Šå¯è½¬æ¢ä¸ºæ•°å€¼
                    quality_report["data_quality_issues"]["data_type_issues"].append({
                        "column": col,
                        "issue": "å¯èƒ½åº”è¯¥æ˜¯æ•°å€¼ç±»å‹",
                        "convertible_percentage": round(numeric_convertible / len(df) * 100, 2)
                    })
        
        # ç”Ÿæˆå»ºè®®
        quality_report["recommendations"] = self._generate_recommendations(quality_report)
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        quality_report["overall_score"] = self._calculate_quality_score(quality_report, total_cells, missing_cells)
        
        self.quality_report = quality_report
        print(f"âœ… æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆï¼Œæ€»ä½“è¯„åˆ†: {quality_report['overall_score']}/100")
        
        return quality_report
    
    def _check_format_consistency(self, series: pd.Series, col_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ ¼å¼ä¸€è‡´æ€§"""
        if col_name.lower() in ['email', 'é‚®ç®±']:
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            valid_emails = series.dropna().str.match(email_pattern).sum()
            total_emails = len(series.dropna())
            if total_emails > 0 and valid_emails / total_emails < 0.8:
                return {
                    "valid_count": valid_emails,
                    "total_count": total_emails,
                    "valid_percentage": round(valid_emails / total_emails * 100, 2),
                    "issue": "é‚®ç®±æ ¼å¼ä¸ä¸€è‡´"
                }
        
        elif col_name.lower() in ['phone', 'ç”µè¯']:
            # ç®€å•çš„ç”µè¯å·ç æ£€æŸ¥
            phone_pattern = r'^[\d\-\+\(\)\s]{7,}$'
            valid_phones = series.dropna().str.match(phone_pattern).sum()
            total_phones = len(series.dropna())
            if total_phones > 0 and valid_phones / total_phones < 0.8:
                return {
                    "valid_count": valid_phones,
                    "total_count": total_phones,
                    "valid_percentage": round(valid_phones / total_phones * 100, 2),
                    "issue": "ç”µè¯æ ¼å¼ä¸ä¸€è‡´"
                }
        
        return None
    
    def _generate_recommendations(self, quality_report: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ•°æ®æ¸…æ´—å»ºè®®"""
        recommendations = []
        
        # ç¼ºå¤±å€¼å»ºè®®
        missing_issues = quality_report["data_quality_issues"]["missing_values"]
        if missing_issues:
            high_missing_cols = [col for col, info in missing_issues.items() if info["percentage"] > 50]
            if high_missing_cols:
                recommendations.append(f"è€ƒè™‘åˆ é™¤ç¼ºå¤±å€¼è¶…è¿‡50%çš„åˆ—: {', '.join(high_missing_cols)}")
            
            moderate_missing_cols = [col for col, info in missing_issues.items() if 10 < info["percentage"] <= 50]
            if moderate_missing_cols:
                recommendations.append(f"å¯¹ç¼ºå¤±å€¼è¾ƒå¤šçš„åˆ—è¿›è¡Œå¡«å……å¤„ç†: {', '.join(moderate_missing_cols)}")
        
        # é‡å¤è¡Œå»ºè®®
        if quality_report["data_quality_issues"]["duplicate_rows"]["count"] > 0:
            recommendations.append("åˆ é™¤é‡å¤è¡Œä»¥æé«˜æ•°æ®è´¨é‡")
        
        # æ•°æ®ç±»å‹å»ºè®®
        type_issues = quality_report["data_quality_issues"]["data_type_issues"]
        if type_issues:
            recommendations.append("è½¬æ¢æ•°æ®ç±»å‹ä»¥æé«˜åˆ†ææ•ˆç‡")
        
        # å¼‚å¸¸å€¼å»ºè®®
        outliers = quality_report["data_quality_issues"]["outliers"]
        if outliers:
            recommendations.append("æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼")
        
        # æ ¼å¼ä¸€è‡´æ€§å»ºè®®
        format_issues = quality_report["data_quality_issues"]["inconsistent_formats"]
        if format_issues:
            recommendations.append("æ ‡å‡†åŒ–æ•°æ®æ ¼å¼")
        
        return recommendations
    
    def _calculate_quality_score(self, quality_report: Dict[str, Any], total_cells: int, missing_cells: int) -> int:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        score = 100
        
        # ç¼ºå¤±å€¼æ‰£åˆ†
        missing_percentage = missing_cells / total_cells * 100
        score -= min(missing_percentage * 0.5, 30)  # æœ€å¤šæ‰£30åˆ†
        
        # é‡å¤è¡Œæ‰£åˆ†
        duplicate_percentage = quality_report["data_quality_issues"]["duplicate_rows"]["percentage"]
        score -= min(duplicate_percentage * 0.3, 15)  # æœ€å¤šæ‰£15åˆ†
        
        # æ•°æ®ç±»å‹é—®é¢˜æ‰£åˆ†
        type_issues_count = len(quality_report["data_quality_issues"]["data_type_issues"])
        score -= min(type_issues_count * 5, 20)  # æœ€å¤šæ‰£20åˆ†
        
        # å¼‚å¸¸å€¼æ‰£åˆ†
        outliers_count = len(quality_report["data_quality_issues"]["outliers"])
        score -= min(outliers_count * 3, 15)  # æœ€å¤šæ‰£15åˆ†
        
        # æ ¼å¼é—®é¢˜æ‰£åˆ†
        format_issues_count = len(quality_report["data_quality_issues"]["inconsistent_formats"])
        score -= min(format_issues_count * 5, 20)  # æœ€å¤šæ‰£20åˆ†
        
        return max(int(score), 0)
    
    def clean_data(self, df: pd.DataFrame, cleaning_options: Dict[str, Any] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """æ™ºèƒ½æ•°æ®æ¸…æ´—"""
        print("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        if cleaning_options is None:
            cleaning_options = {
                "remove_duplicates": True,
                "handle_missing": "auto",  # auto, drop, fill
                "fix_data_types": True,
                "remove_outliers": False,
                "standardize_text": True,
                "missing_threshold": 50  # ç¼ºå¤±å€¼è¶…è¿‡æ­¤ç™¾åˆ†æ¯”çš„åˆ—å°†è¢«åˆ é™¤
            }
        
        cleaned_df = df.copy()
        cleaning_log = {
            "original_shape": df.shape,
            "operations": [],
            "final_shape": None,
            "summary": {}
        }
        
        # 1. åˆ é™¤é‡å¤è¡Œ
        if cleaning_options.get("remove_duplicates", True):
            before_count = len(cleaned_df)
            cleaned_df = cleaned_df.drop_duplicates()
            removed_count = before_count - len(cleaned_df)
            if removed_count > 0:
                cleaning_log["operations"].append(f"åˆ é™¤äº† {removed_count} è¡Œé‡å¤æ•°æ®")
                print(f"ğŸ—‘ï¸ åˆ é™¤äº† {removed_count} è¡Œé‡å¤æ•°æ®")
        
        # 2. å¤„ç†ç¼ºå¤±å€¼
        missing_handling = cleaning_options.get("handle_missing", "auto")
        missing_threshold = cleaning_options.get("missing_threshold", 50)
        
        for col in cleaned_df.columns:
            missing_percentage = cleaned_df[col].isnull().sum() / len(cleaned_df) * 100
            
            if missing_percentage > missing_threshold:
                if missing_handling == "auto" or missing_handling == "drop_columns":
                    cleaned_df = cleaned_df.drop(columns=[col])
                    cleaning_log["operations"].append(f"åˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„åˆ—: {col} ({missing_percentage:.1f}%)")
                    print(f"ğŸ—‘ï¸ åˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„åˆ—: {col}")
                    continue
            
            if missing_percentage > 0:
                if missing_handling == "auto":
                    # æ™ºèƒ½å¡«å……
                    if cleaned_df[col].dtype in ['int64', 'float64']:
                        # æ•°å€¼åˆ—ç”¨ä¸­ä½æ•°å¡«å……
                        fill_value = cleaned_df[col].median()
                        cleaned_df[col] = cleaned_df[col].fillna(fill_value)
                        cleaning_log["operations"].append(f"ç”¨ä¸­ä½æ•°å¡«å……åˆ— {col} çš„ç¼ºå¤±å€¼")
                    else:
                        # æ–‡æœ¬åˆ—ç”¨ä¼—æ•°å¡«å……
                        mode_value = cleaned_df[col].mode()
                        if len(mode_value) > 0:
                            cleaned_df[col] = cleaned_df[col].fillna(mode_value[0])
                            cleaning_log["operations"].append(f"ç”¨ä¼—æ•°å¡«å……åˆ— {col} çš„ç¼ºå¤±å€¼")
                        else:
                            cleaned_df[col] = cleaned_df[col].fillna("æœªçŸ¥")
                            cleaning_log["operations"].append(f"ç”¨'æœªçŸ¥'å¡«å……åˆ— {col} çš„ç¼ºå¤±å€¼")
                elif missing_handling == "drop":
                    before_count = len(cleaned_df)
                    cleaned_df = cleaned_df.dropna(subset=[col])
                    removed_count = before_count - len(cleaned_df)
                    if removed_count > 0:
                        cleaning_log["operations"].append(f"åˆ é™¤äº†åˆ— {col} ä¸­æœ‰ç¼ºå¤±å€¼çš„ {removed_count} è¡Œ")
        
        # 3. ä¿®å¤æ•°æ®ç±»å‹
        if cleaning_options.get("fix_data_types", True):
            for col in cleaned_df.columns:
                if cleaned_df[col].dtype == 'object':
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                    numeric_series = pd.to_numeric(cleaned_df[col], errors='coerce')
                    if numeric_series.notna().sum() > len(cleaned_df) * 0.8:
                        cleaned_df[col] = numeric_series
                        cleaning_log["operations"].append(f"å°†åˆ— {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
                        print(f"ğŸ”„ å°†åˆ— {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
                    
                    # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸç±»å‹
                    elif col.lower() in ['date', 'æ—¥æœŸ', 'time', 'æ—¶é—´', 'created', 'updated']:
                        try:
                            date_series = pd.to_datetime(cleaned_df[col], errors='coerce')
                            if date_series.notna().sum() > len(cleaned_df) * 0.8:
                                cleaned_df[col] = date_series
                                cleaning_log["operations"].append(f"å°†åˆ— {col} è½¬æ¢ä¸ºæ—¥æœŸç±»å‹")
                                print(f"ğŸ“… å°†åˆ— {col} è½¬æ¢ä¸ºæ—¥æœŸç±»å‹")
                        except:
                            pass
        
        # 4. æ ‡å‡†åŒ–æ–‡æœ¬
        if cleaning_options.get("standardize_text", True):
            for col in cleaned_df.columns:
                if cleaned_df[col].dtype == 'object':
                    # å»é™¤é¦–å°¾ç©ºæ ¼
                    cleaned_df[col] = cleaned_df[col].astype(str).str.strip()
                    
                    # ç»Ÿä¸€å¤§å°å†™ï¼ˆå¦‚æœæ˜¯è‹±æ–‡ä¸”å€¼è¾ƒå°‘ï¼‰
                    if cleaned_df[col].nunique() < len(cleaned_df) * 0.1:  # å¦‚æœå”¯ä¸€å€¼å°‘äº10%
                        if cleaned_df[col].str.contains(r'^[a-zA-Z\s]+$', na=False).any():
                            original_values = cleaned_df[col].unique()
                            cleaned_df[col] = cleaned_df[col].str.title()
                            new_values = cleaned_df[col].unique()
                            if len(original_values) != len(new_values):
                                cleaning_log["operations"].append(f"æ ‡å‡†åŒ–åˆ— {col} çš„æ–‡æœ¬æ ¼å¼")
        
        # 5. æ¸…ç†åˆ—å
        original_columns = list(cleaned_df.columns)
        cleaned_df.columns = [self._clean_column_name(col) for col in cleaned_df.columns]
        if list(cleaned_df.columns) != original_columns:
            cleaning_log["operations"].append("æ¸…ç†äº†åˆ—åæ ¼å¼")
            print("ğŸ§¹ æ¸…ç†äº†åˆ—åæ ¼å¼")
        
        cleaning_log["final_shape"] = cleaned_df.shape
        cleaning_log["summary"] = {
            "rows_removed": df.shape[0] - cleaned_df.shape[0],
            "columns_removed": df.shape[1] - cleaned_df.shape[1],
            "operations_count": len(cleaning_log["operations"])
        }
        
        self.cleaning_log = cleaning_log
        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {df.shape} â†’ {cleaned_df.shape}")
        
        return cleaned_df, cleaning_log
    
    def _clean_column_name(self, col_name: str) -> str:
        """æ¸…ç†åˆ—å"""
        cleaned = str(col_name).strip()
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '_', cleaned)
        # åˆå¹¶å¤šä¸ªä¸‹åˆ’çº¿
        cleaned = re.sub(r'_+', '_', cleaned)
        # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
        cleaned = cleaned.strip('_')
        return cleaned or 'unnamed_column'
    
    def generate_processing_report(self, quality_report: Dict[str, Any], cleaning_log: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®å¤„ç†æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "data_quality": quality_report,
            "cleaning_operations": cleaning_log,
            "summary": {
                "original_quality_score": quality_report.get("overall_score", 0),
                "processing_operations": len(cleaning_log.get("operations", [])),
                "data_reduction": {
                    "rows": cleaning_log.get("summary", {}).get("rows_removed", 0),
                    "columns": cleaning_log.get("summary", {}).get("columns_removed", 0)
                }
            },
            "recommendations_implemented": [],
            "remaining_issues": []
        }
        
        # åˆ†æå·²å®æ–½çš„å»ºè®®
        operations = cleaning_log.get("operations", [])
        recommendations = quality_report.get("recommendations", [])
        
        for rec in recommendations:
            if any(op in rec for op in ["åˆ é™¤é‡å¤", "åˆ é™¤ç¼ºå¤±", "è½¬æ¢", "æ ‡å‡†åŒ–"]):
                if any(similar_op in ' '.join(operations) for similar_op in ["åˆ é™¤", "è½¬æ¢", "æ ‡å‡†åŒ–"]):
                    report["recommendations_implemented"].append(rec)
                else:
                    report["remaining_issues"].append(rec)
        
        return report
    
    def preview_data(self, df: pd.DataFrame, n_rows: int = 10) -> Dict[str, Any]:
        """é¢„è§ˆæ•°æ®"""
        preview = {
            "shape": df.shape,
            "columns": list(df.columns),
            "dtypes": df.dtypes.to_dict(),
            "head": df.head(n_rows).to_dict('records'),
            "tail": df.tail(n_rows).to_dict('records'),
            "sample": df.sample(min(n_rows, len(df))).to_dict('records') if len(df) > n_rows else [],
            "basic_stats": {}
        }
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            preview["basic_stats"]["numeric"] = df[numeric_cols].describe().to_dict()
        
        text_cols = df.select_dtypes(include=['object']).columns
        if len(text_cols) > 0:
            preview["basic_stats"]["text"] = {}
            for col in text_cols:
                preview["basic_stats"]["text"][col] = {
                    "unique_count": df[col].nunique(),
                    "most_common": df[col].value_counts().head(5).to_dict()
                }
        
        return preview 